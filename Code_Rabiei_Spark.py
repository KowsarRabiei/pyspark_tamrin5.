# -*- coding: utf-8 -*-
"""spark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R-7cfByRcjwZFdcVEyEiES2gU2isOc-N
"""

!pip install pyspark

from pyspark.sql import SparkSession
spark=SparkSession.builder.appName('my_app').getOrCreate()
df=spark.read.csv('/content/ML_hw_dataset.csv',header=True,inferSchema=True)

df.show()

df.describe().show()

from pyspark.sql.functions import isnan,when,count,col
df.select([count(when(isnan(i)|col(i).isNull(),i)).alias(i) for i in df.columns]).show()

all_null=0
for i in df.columns:
  null_count=df.where(col(i).isNull()).count()
  print(f"column '{i}' has {null_count} null values.")
  
  all_null+=null_count
print('all data missing is:',all_null)

#df=df.replace('unknown',None)

df=df.dropna()

df.show()

from pyspark.ml.feature import StringIndexer
indexer=StringIndexer(inputCol='',outputCol='indexed')
string_cols = [col_name for col_name, col_type in df.dtypes if col_type == "string"]
#for col in string_cols:
out_cols = [c+'_index' for c in string_cols]
indexer = StringIndexer(inputCols=string_cols, outputCols=out_cols)
df=indexer.fit(df).transform(df)

from pyspark.sql.functions import col

numeric_cols=[c for c ,t in df.dtypes if t in ['int','double','float','long']]

for col_name in numeric_cols:
  q1=df.approxQuantile(col_name,[0.25],0.01)[0]
  q3=df.approxQuantile(col_name,[0.75],0.01)[0]
  iqr=q3-q1

  k=1.5
  lower_bound=q1-k*iqr
  upper_bound=q3+k*iqr
  
  df=df.filter((col(col_name) >= lower_bound) & (col(col_name)<=upper_bound))

from pyspark.sql.functions import monotonically_increasing_id
from pyspark.ml.feature import VectorAssembler,StandardScaler,MinMaxScaler

spark = SparkSession.builder.appName("Normalization").getOrCreate()

numeric_cols = [col for col, dtype in df.dtypes if dtype == "int" or dtype == "double"]
df_numeric = df.select(numeric_cols)

assembler = VectorAssembler(inputCols=numeric_cols, outputCol="features")
df_numeric_assembled = assembler.transform(df_numeric).select("features")

scaler = MinMaxScaler(inputCol="features", outputCol="scaled_features")
scaler_model = scaler.fit(df_numeric_assembled)
df_normalized = scaler_model.transform(df_numeric_assembled)

df_normalized = df_normalized.withColumn("id", monotonically_increasing_id())
df = df.withColumn("id", monotonically_increasing_id())

df_normalized = df_normalized.join(df, "id", "outer").drop(df_normalized["id"])
df_normalized.show(10)

from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.stat import Correlation
import seaborn as sns
import matplotlib.pyplot as plt

numeric_cols = [c for c,d in df.dtypes if d in ['int','double']]
df.select(numeric_cols).show()
assembler = VectorAssembler(inputCols=numeric_cols, outputCol='features')
assembled_data = assembler.transform(df).select('features')

correlation_matrix = Correlation.corr(assembled_data, 'features').head()
corr_array = correlation_matrix[0].toArray()

col_names = [c for c in numeric_cols]

sns.set(font_scale=1.2)
fig, ax = plt.subplots(figsize=(10,10))
sns.heatmap(corr_array, cmap='cool', annot=True, square=True, xticklabels=col_names, yticklabels=col_names, ax=ax)

plt.show()

# گام دوم استاندارد سازی داده ها
from pyspark.ml.linalg import Vectors
from pyspark.ml.feature import VectorAssembler
assembler = VectorAssembler(
    inputCols=numeric_cols,
    outputCol='features')

proccess_df = assembler.transform(df).select("features","y")
from pyspark.ml.feature import StandardScaler

# Create a StandardScaler instance
scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures")

# Fit the scaler to the data and transform
scaledData = scaler.fit(proccess_df).transform(proccess_df)

#گام دوم pca و کاهش ابعاد
from pyspark.ml.feature import PCA
from pyspark.ml.linalg import Vectors
assembler = VectorAssembler(
    inputCols=numeric_cols,
    outputCol='features')

proccess_df = assembler.transform(df).select("features","y")
k = 15

pca = PCA(k=k, inputCol="features", outputCol="pcaFeatures")

model = pca.fit(proccess_df)
pca_result = model.transform(proccess_df).select("pcaFeatures","y")

pca_result.show(1,truncate=False)

#گام دوم 

from pyspark.sql.functions import corr
df_normalized.select([corr('age', 'y').alias('corr_age_y'),
           corr('pdays', 'y').alias('corr_pdays_y'),
           corr('emp_var_rate', 'y').alias('corr_emp_var_rate_y'),
           corr('cons_price_idx', 'y').alias('corr_cons_price_idx_y'),
           corr('cons_conf_idx', 'y').alias('corr_cons_conf_idx_y'),
           corr('euribor3m', 'y').alias('corr_euribor3m_y'),
           corr('job_index', 'y').alias('corr_job_index_y'),
           corr('marital_index', 'y').alias('corr_marital_index_y'),
           corr('education_index', 'y').alias('corr_education_index_y'),
           corr('default_index', 'y').alias('corr_default_index_y'),
           corr('housing_index', 'y').alias('corr_housing_index_y'),
           corr('contact_index', 'y').alias('corr_contact_index_y'),
           corr('month_index', 'y').alias('corr_month_index_y'),
           corr('day_of_week_index', 'y').alias('corr_day_of_week_index_y'),
           corr('poutcome_index', 'y').alias('corr_dpoutcome_index_y')])

from pyspark.sql.functions import col
from pyspark.ml.feature import PCA
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline


spark = SparkSession.builder.appName('my_app').getOrCreate()
# Load the CSV file into a DataFrame
df = spark.read.csv("/content/ML_hw_dataset.csv", header=True, inferSchema=True)

# Identify the string columns and convert them to numeric using StringIndexer
string_cols = [col_name for col_name, col_type in df.dtypes if col_type == 'string']
indexers = [StringIndexer(inputCol=col_name, outputCol=col_name + "_indexed") for col_name in string_cols]
indexer_pipeline = Pipeline(stages=indexers)
df_indexed = indexer_pipeline.fit(df).transform(df)

# Create a VectorAssembler to combine all the features into a single vector
numeric_cols = [col_name for col_name, col_type in df_indexed.dtypes if col_type != 'string' and col_name != "y"]
assembler = VectorAssembler(inputCols=numeric_cols, outputCol="features")
assembled_df = assembler.transform(df_indexed)

# Apply PCA to the assembled_df with k=15
pca = PCA(k=20, inputCol="features", outputCol="pca_features")
model = pca.fit(assembled_df)
pca_df = model.transform(assembled_df).select("pca_features","y")
pca_df.show()

# Split the data into training and testing sets
train_data, test_data = pca_df.randomSplit([0.8, 0.2], seed=100)

# Define the logistic regression model
lr = LogisticRegression(labelCol='y', featuresCol='pca_features')

# Fit the model to the training data
lr_model = lr.fit(train_data)

# Make predictions on the test data
predictions = lr_model.transform(test_data)

from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator

# Evaluate the model's performance
evaluator = BinaryClassificationEvaluator(labelCol='y', rawPredictionCol='prediction')
auc = evaluator.evaluate(predictions)

print(lr_model.evaluate(test_data).recallByLabel)

print(lr_model.evaluate(test_data).accuracy)

print(lr_model.evaluate(test_data).precisionByLabel)